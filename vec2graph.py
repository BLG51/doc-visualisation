# -*- coding: utf-8 -*-
"""vec2graph.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1iKXPxsOXesyxqW1z0yfl5vw7AB_g-I8R
"""

!pip install tika

!pip install pymorphy2

!pip install -U pymorphy2-dicts-ru

!pip install vec2graph

from tika import parser  
import os
from os import listdir
from os.path import isfile, join
import re
import nltk
nltk.download('punkt')
from nltk.corpus import stopwords
nltk.download('stopwords')
import pymorphy2
import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import TruncatedSVD
from gensim import models
from gensim import utils
import logging
import argparse
import zipfile
from vec2graph import visualize
import json
import scipy
from scipy import sparse
from sklearn.cluster import KMeans
from glob import glob

from google.colab import drive
drive.mount('/content/drive')

def get_filepaths(directory):
    file_paths = []  # List which will store all of the full filepaths.

    # Walk the tree.
    for root, directories, files in os.walk(directory):
        for filename in files:
            # Join the two strings in order to form the full filepath.
            filepath = os.path.join(root, filename)
            file_paths.append(filepath)  # Add it to the list.

    return file_paths  # Self-explanatory.

def lineWithoutStopWords(line):
    stopwords_path = join('content/drive/MyDrive/stopwords')
    stopwords_template = join(stopwords_path, '*.txt')
    stopwords_files = glob(stopwords_template)
    extra_stopwords = set()
    for name in stopwords_files:
      with open(name, 'r') as f:
        words = f.readlines()
        words = [word.strip() for word in words]
        extra_stopwords.update(set(words))
    sto = set()
    sto.update(set(stopwords.words("russian")))
    sto.update(extra_stopwords)    
    return [word for word in line if word not in sto]

def normalForm(line):
    morph = pymorphy2.MorphAnalyzer()
    return [morph.parse(word)[0].normal_form for word in line if len(word)>2]

corpus= []
for path in get_filepaths('/content/drive/MyDrive/mathcenter'):
  parsed_pdf = parser.from_file(path)
  text = parsed_pdf['content'] 
  if text != None:
    text = text.lower()
    text = re.sub('[^A-Za-zА-Яа-я\s]',' ',text,flags=re.UNICODE)
    tokenizedText = nltk.word_tokenize(text)
    withoutStopWords = lineWithoutStopWords(tokenizedText)
    normal = normalForm(withoutStopWords)
    corpus.append([path, normal])

parsed_pdf = parser.from_file('/content/drive/MyDrive/mathcenter/1/1-pp. 101-102.pdf')
text = parsed_pdf['content'] 
text = text.lower() #/([а-я]+)/ui 
#text = re.sub(r'[^\w\s]',' ',text,flags=re.UNICODE)
text = re.sub('[^A-Za-zА-Яа-я\s]',' ',text,flags=re.UNICODE)
tokenizedText = nltk.word_tokenize(text)
withoutStopWords = lineWithoutStopWords(tokenizedText)
print (withoutStopWords)
normal = normalForm(withoutStopWords)
print(normal)

with open("output.txt", "w") as txt_file:
  for row in corpus:
          txt_file.write(" ".join(str(x) for x in row) + "\n")

#parsed_pdf = parser.from_file(f for f in listdir('/content/drive/MyDrive/mathcenter'))

with open('/content/drive/MyDrive/dip/output.txt', 'r') as f:
    lines = f.readlines()

lines[0]

corpus= []

corpus.append( eval ('[' + lines[0].partition('[')[2].strip() ))

for line in lines:
  corpus.append( eval ('[' + line.partition('[')[2].strip() ))

corpus[3]

tdata = np.array(corpus)

def dummy_fun(doc):
    return doc

tdata

tf_idf_t = TfidfVectorizer(
    analyzer='word',
    tokenizer=dummy_fun,
    preprocessor=dummy_fun,
    token_pattern=None)  
tf_idf_t.fit(tdata)
#getting idfs
idfs = tf_idf_t.idf_
#sorting out too rare and too common words
#original 1.3 and 7
# 2 6
lower_thresh = 3.
upper_thresh = 8.
not_often = idfs > lower_thresh
not_rare = idfs < upper_thresh

mask = not_often * not_rare

good_words = np.array(tf_idf_t.get_feature_names())[mask]
#deleting punctuation as well.
cleaned = []
for word in good_words:
    word = re.sub("^(\d+\w*$|_+)", "", word)
    
    if len(word) == 0:
        continue
    cleaned.append(word)
print("Len of original vocabulary: %d\nAfter filtering: %d"%(idfs.shape[0], len(cleaned)))

corpus2 = corpus

for i in corpus2:
  for j in i:

cleaned

lines[0].partition('[')[0].strip()

lines[0].partition('[')[0].replace(" ","")

with open('paths.txt', 'w') as f:
  for line in lines:
    stre = line.partition('[')[0].strip()
    f.write(stre+"\n")

voc = {word : i for i,word in enumerate(cleaned)}

cv = CountVectorizer(
                             vocabulary=voc)

from sklearn.model_selection import train_test_split

train_names, test_names = train_test_split(cleaned, test_size=0.1, random_state=666)

dataset = cv.fit_transform(train_names)

tfidf = TfidfVectorizer(
    analyzer='word',
    tokenizer=dummy_fun,
    preprocessor=dummy_fun,
    token_pattern=None)

data = np.array(corpus)

x=tfidf.fit_transform(data[:,1])

x=tfidf.fit_transform(data[:,1])

x = np.round(x.toaarray(), 5)

x.toarray()

x = np.load('/content/drive/MyDrive/dip/tfidf.txt.npy')

xa = x.toarray()

xa = np.round(x, 5)

from sklearn.decomposition import LatentDirichletAllocation as LDA



SVD = TruncatedSVD(100)

SVD = TruncatedSVD(4)

xa = SVD.fit_transform(x)

xa = x

xa.shape

xa = sparse.csr_matrix(SVD.fit_transform(x))

lda = LDA(n_components = 60, max_iter=3, n_jobs=6, learning_method='batch', verbose=1)
lda.fit(dataset)

vv = cv.transform(lineWithoutStopWords(lines[5]))

import gensim

import gensim.corpora as corpora
from gensim.models import LsiModel
from gensim.models.coherencemodel import CoherenceModel
import matplotlib.pyplot as plt

voc = {word : i for i,word in enumerate(cleaned)}]

corpus3 = []

for i in corpus2:
  j= [x for x in i if x in cleaned]
  corpus3.append(j)

len(corpus)



def prepare_corpus(doc_clean):
    """
    Input  : clean document
    Purpose: create term dictionary of our courpus and Converting list of documents (corpus) into Document Term Matrix
    Output : term dictionary and Document Term Matrix
    """
    # Creating the term dictionary of our courpus, where every unique term is assigned an index. dictionary = corpora.Dictionary(doc_clean)
    dictionary = corpora.Dictionary(doc_clean)
    # Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.
    doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]
    # generate LDA model
    return dictionary,doc_term_matrix

def compute_coherence_values(dictionary, doc_term_matrix, doc_clean, stop, start=2, step=3):
    """
    Input   : dictionary : Gensim dictionary
              corpus : Gensim corpus
              texts : List of input texts
              stop : Max num of topics
    purpose : Compute c_v coherence for various number of topics
    Output  : model_list : List of LSA topic models
              coherence_values : Coherence values corresponding to the LDA model with respective number of topics
    """
    coherence_values = []
    model_list = []
    for num_topics in range(start, stop, step):
        # generate LSA model
        model = LsiModel(doc_term_matrix, num_topics=num_topics, id2word = dictionary)  # train model
        model_list.append(model)
        coherencemodel = CoherenceModel(model=model, texts=doc_clean, dictionary=dictionary, coherence='c_v')
        coherence_values.append(coherencemodel.get_coherence())
    return model_list, coherence_values

def plot_graph(doc_clean,start, stop, step):
    dictionary,doc_term_matrix=prepare_corpus(doc_clean)
    model_list, coherence_values = compute_coherence_values(dictionary, doc_term_matrix,doc_clean,
                                                            stop, start, step)
    # Show graph
    x = range(start, stop, step)
    plt.plot(x, coherence_values)
    plt.xlabel("Number of Topics")
    plt.ylabel("Coherence score")
    plt.legend(("coherence_values"), loc='best')
    plt.show()

start,stop,step=2,12,2
plot_graph(corpus3,start,stop,step)



di = corpora.Dictionary(corpus3)
    # Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.
dm = [di.doc2bow(doc) for doc in corpus3]


lda_model = gensim.models.LdaMulticore(corpus=dm,
                                       id2word=di,
                                       num_topics=6)
# Print the Keyword in the 10 topics
print(lda_model.print_topics())

data3 = np.array(corpus3)
x3=tfidf.fit_transform(data3)

x3.shape

SVD = TruncatedSVD(6)

xa3 = SVD.fit_transform(x3)

lda.transform(vv)

xa

xa.shape

xa.shape

xa[1]

np.save("tfidf.txt", xa)

np.save("tfidf.txt", xa3)

str

with open('/content/drive/MyDrive/dip/paths.txt', 'r') as f:
    pathnames = f.readlines()

pathnames[0]

with open('/content/drive/MyDrive/ИТОГ/6. article_files.sql', 'r') as f:
  af=f.readlines()

afd={}
k=0
for aline in af:
  p1 = re.compile('[a-z]+\/.+?pdf')
  p2 = re.compile('1\d+')
  if p2.search(aline)!=None and p1.search(aline)!=None:
      afd[p2.search(aline).group(0)] = p1.search(aline).group(0)

with open ('/content/drive/MyDrive/ИТОГ/4. articles_titles.sql', 'r') as f:
  af=f.readlines()

k=0
afd1={}
afd2={}
for aline in af:
  p1 = re.compile('\',\'\s?(.+)\'\)')
  p2 = re.compile('1\d+')
  if p2.search(aline)!=None and p1.search(aline)!=None:
      afd1[p2.search(aline).group(0)] = p1.search(aline).group(1)
      afd2[afd[p2.search(aline).group(0)]] = afd1[p2.search(aline).group(0)]

afd2

with open('names.txt', 'w') as f:
  for line in pathnames:
    line = re.sub('/content/drive/MyDrive/mathcenter/\d+(,\d)?/', 'public/collections/', line)
    f.write(afd2[line.strip()]+'\n')

with open('names.txt', 'r') as f:
  names = f.readlines()

np.savetxt("tfidf.txt", xa)

np.savetxt("tfidf.txt", xa3)

np.savetxt("tfidf.txt", xa, fmt='%1.5f')

with open('/content/drive/MyDrive/dip/tfidf.txt', 'r') as f:
  vectorlines = f.readlines()

with open('tfidf.txt', 'r') as f:
  vectorlines = f.readlines()

vectorlines[0]

len(vectorlines)

xa.shape

xa3.shape

xa

for el in xa[0]:
  if el>0:
    print(el)

with open('ef1.txt', 'w') as f:
    f.write(" ".join((str(xa.shape[0]),str(xa.shape[1])))+ "\n")
    k=0
    while(k<2416): #2416
      f.write(names[k].replace(" ","_").strip() + " " + vectorlines[k].strip() + "\n")
      k+=1

with open('ef1.txt', 'w') as f:
    f.write(" ".join((str(xa3.shape[0]),str(xa3.shape[1])))+ "\n")
    k=0
    while(k<2416): #2416
      f.write(names[k].replace(" ","_").strip() + " " + vectorlines[k].strip() + "\n")
      k+=1

with open('ef1.txt', 'w') as f:
    f.write(" ".join((str(xa.shape[0]),str(xa.shape[1])))+ "\n")
    k=0
    while(k<2416): #2416
      f.write(names[k].replace(" ","_").strip() + " " + vectorlines[k].strip() + "\n")
      k+=1

with open('ef.txt', 'r') as f:
    eflines = f.readlines()

with open('ef1.txt', 'r') as f:
    eflines = f.readlines()

streeng

streeng = eflines[1]

streeng2 = utils.to_unicode(streeng.rstrip(), encoding='utf8', errors='relpace').split(" ")

streeng.count('.')

len(streeng2)

!wget http://nlp.stanford.edu/data/glove.6B.zip

!unzip glove*.zip

def load_embeddings(embeddings_file):
    # Detect the model format by its extension:
    # Binary word2vec format:
    if embeddings_file.endswith('.bin.gz') or embeddings_file.endswith('.bin'):
        emb_model = models.KeyedVectors.load_word2vec_format(embeddings_file, binary=True,
                                                             unicode_errors='replace')
    # Text word2vec format:
    elif embeddings_file.endswith('.txt.gz') or embeddings_file.endswith('.txt') \
            or embeddings_file.endswith('.vec.gz') or embeddings_file.endswith('.vec'):
        emb_model = models.KeyedVectors.load_word2vec_format(
            embeddings_file, binary=False, unicode_errors='replace')
    # ZIP archive from the NLPL vector repository:
    elif embeddings_file.endswith('.zip'):
        with zipfile.ZipFile(embeddings_file, "r") as archive:
            # Loading and showing the metadata of the model:
            metafile = archive.open('meta.json')
            metadata = json.loads(metafile.read())
            for key in metadata:
                print(key, metadata[key])
            print('============')
            # Loading the model itself:
            stream = archive.open("model.bin")  # or model.txt, if you want to look at the model
            emb_model = models.KeyedVectors.load_word2vec_format(
                stream, binary=True, unicode_errors='replace')
    else:
        # Native Gensim format?
        emb_model = models.KeyedVectors.load(embeddings_file)
        # If you intend to train further: emb_model = models.Word2Vec.load(embeddings_file)

    emb_model.init_sims(replace=True)  # Unit-normalizing the vectors (if they aren't already)
    return emb_model

logging.basicConfig(
        format="%(asctime)s : %(levelname)s : %(message)s", level=logging.INFO
    )

parser = argparse.ArgumentParser()

parser.add_argument(
        "-w",
        "--word",
        help="word to look for in the model. If omitted, random word is used",
        default="car",
    )
parser.add_argument(
        "-n",
        "--nbr",
        help="amount of neighbors to show.",
        default=10,
        type=int,
    )
parser.add_argument(
        "-e",
        "--edge",
        help="width of an edge (link) between nodes.",
        default=1,
        type=int,
    )
parser.add_argument(
        "-d",
        "--depth",
        help="recursion depth to build graphs also of neighbors of target word."
             " Default is 0 (no neighbors)",
        default=0,
        type=int,
    )
parser.add_argument(
        "-l",
        "--lim",
        help="limit (threshold) of cosine similarity which should be surpassed by"
             " neighbors to be rendered as connected. Scale is either more "
             "than 0 and less than 1 (as real range for similarities), or"
             " from 1 to 100 as percents. Default is 0 (all edges are preserved)",
        default=0,
        type=float,
    )
parser.add_argument(
        "-m",
        "--model",
        help="path to vector model file. If omitted, first model with the extension "
             "bin.gz (as binary) or .vec.gz (as non-binary) in working directory"
             " is loaded",
        default="glove.6B.300d.w2vformat.txt",
    )


parser.add_argument(
        "-s",
        "--sep",
        help="if this parameter is used, the words are split by a separator"
             "(underscore), and only the first part is shown in visualization (E.g. "
             "it is useful when PoS is attached to a word). By now, this "
             "parameter accepts no value",
        action="store_true",
    )

parser.add_argument(
        "-js",
        "--javascript",
        help="path to D3.js library, can be 'web' (link to version at the D3.js "
             "site) or 'local' (file in the directory with generated HTML, if not"
             " present, it is downloaded from web). Default is 'web'",
        choices=("web", "local"),
        default="web",
    )
parser.add_argument("-f", "--fff", help="a dummy argument to fool ipython", default="1")

parser.add_argument(
        "-o",
        "--output",
        help="path to the output directory where to store visualization files."
             " If omitted, a new directory will be made in the current one, with the name"
             " based on the timestamp",
        default="newdir",
    )

args = parser.parse_args()

!python -m gensim.scripts.glove2word2vec --input  glove.6B.300d.txt --output glove.6B.300d.w2vformat.txt



model = load_embeddings(args.model)

sbeve = streeng.rstrip().split(" ")

len(sbeve)

sbeve[0]

sbeve[1:]

sbeve[71018]

coo = 0
coco = 0
for el in sbeve[1:]:
  if float(el)==0 or float(el)>0:
    coo+=1
  else:
    coco+=1
coco

!jupyter notebook --NotebookApp.iopub_data_rate_limit=1.0e10

kavo=[]
with utils.smart_open ('ef.txt') as fin:
      encoding='utf-8'
      header = utils.to_unicode(fin.readline(), encoding=encoding)
      print(header)
      vocab_size, vector_size = (int(x) for x in header.split())
      for l in range(5):
        line = fin.readline()
        kavo.append(line)
        parts = utils.to_unicode(line.rstrip(), encoding=encoding, errors='replace').split(" ")
        print(len(parts))
print(kavo[1])

with utils.smart_open('ef1.txt') as fin:
      encoding='utf-8'
      header = utils.to_unicode(fin.readline(), encoding=encoding)
      vocab_size, vector_size = (int(x) for x in header.split())  # throws for invalid file format
      for line_no in range(vocab_size):
              line = fin.readline()
              print(line)
              if line == b'':
                  raise EOFError("unexpected end of input; is count incorrect or file otherwise damaged?")
              parts = utils.to_unicode(line.rstrip(), encoding=encoding, errors='replace').split(" ")
              if len(parts) != vector_size + 1:
                  raise ValueError("invalid vector on line %s (is this really the text format?) btw vec n$s len $s" % line_no, vector_size, len(parts), parts[0])

model = load_embeddings('ef1.txt')

model.save_word2vec_format("vectors")

model.save("vectorsnoformat")

with open('vectors') as f:
    lines = f.readlines()

lines[0]

lines[1]

word = 'football' #"args.word"

import random

word = random.choice(names).replace(" ","_").strip()
word



word = random.choice(pathnames).replace(" ","").strip()

visualize(
        args.output,
        model,
        word,
        depth=args.depth,
        topn=args.nbr,
        threshold=args.lim,
        edge=args.edge,
        sep=args.sep,
        library=args.javascript,
    )

visualize(
    args.output,
    model,
    word,
    depth=2,
    topn=10,
    #threshold=args.lim,
    threshold=0.23,
    edge=args.edge,
    sep=args.sep,
    library=args.javascript,
    )

k_means_clustering = KMeans(n_clusters=3, random_state=0).fit(xa)

k_means_clustering

import matplotlib.pyplot as plt
 
#filter rows of original data
filtered_label0 = df[label == 0]
 
#plotting the results
plt.scatter(filtered_label0[:,0] , filtered_label0[:,1])
plt.show()